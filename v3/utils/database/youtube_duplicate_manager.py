#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
YouTubeé‡è¤‡ç™»éŒ²å‹•ç”»ã®ç®¡ç†ãƒ»æ•´ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

åŒã˜ã‚¿ã‚¤ãƒˆãƒ«+ãƒãƒ£ãƒ³ãƒãƒ«åã®å‹•ç”»ãŒè¤‡æ•°ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã€
å„ªå…ˆåº¦ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ã„ã¦ä¿æŒã™ã‚‹ã‚‚ã®ã‚’æ±ºå®šã—ã€ãã‚Œä»¥å¤–ã‚’å‰Šé™¤ã™ã‚‹ã€‚

å„ªå…ˆåº¦ï¼š
1. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆæœ€ã‚‚å„ªå…ˆåº¦ãŒé«˜ã„ï¼‰
2. ãƒ©ã‚¤ãƒ–ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãŒãªã„å ´åˆï¼‰
3. ãƒ—ãƒ¬ãƒŸã‚¢å…¬é–‹ï¼ˆãƒ©ã‚¤ãƒ–ãŒãªã„å ´åˆã§ã€ç¾åœ¨æ™‚åˆ»ä»¥é™ã¾ãŸã¯ãƒ—ãƒ¬ãƒŸã‚¢é–‹å§‹æ™‚åˆ»ã‹ã‚‰10åˆ†ä»¥å†…ï¼‰
4. é€šå¸¸å‹•ç”»ï¼ˆæœ€ã‚‚å„ªå…ˆåº¦ãŒä½ã„ï¼‰
"""

import sqlite3
import logging
from pathlib import Path

logger = logging.getLogger("AppLogger")

__author__ = "mayuneco(mayunya)"
__copyright__ = "Copyright (C) 2025 mayuneco(mayunya)"
__license__ = "GPLv2"


def _get_db_path():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’ç›¸å¯¾ãƒ‘ã‚¹ã‹ã‚‰è§£æ±º"""
    v3_root = Path(__file__).parent.parent.parent
    return v3_root / "data" / "video_list.db"


def check_duplicate_videos(db_path=None, limit=20):
    """
    YouTubeé‡è¤‡ç™»éŒ²å‹•ç”»ã‚’ãƒã‚§ãƒƒã‚¯

    åŒã˜ã‚¿ã‚¤ãƒˆãƒ«+ãƒãƒ£ãƒ³ãƒãƒ«åã§è¤‡æ•°ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å‹•ç”»ã‚’æ¤œå‡ºã—ã¦è¡¨ç¤º

    Args:
        db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•è§£æ±ºï¼‰
        limit: è¡¨ç¤ºä¸Šé™ä»¶æ•°

    Returns:
        dict: {
            'duplicate_groups': [é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆ],
            'same_video_id_duplicates': [åŒä¸€video_idã®é‡è¤‡ãƒªã‚¹ãƒˆ]
        }
    """
    if db_path is None:
        db_path = _get_db_path()

    # v3 ãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆyoutube_dedup_priority ã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ï¼‰
    import sys
    from pathlib import Path
    v3_root = Path(__file__).parent.parent.parent
    if str(v3_root) not in sys.path:
        sys.path.insert(0, str(v3_root))

    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    # ============================================
    # 1. åŒã˜title+channel_nameã§è¤‡æ•°ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å‹•ç”»
    # ============================================
    cursor.execute('''
    SELECT title, channel_name, COUNT(*) as cnt, GROUP_CONCAT(video_id, ',') as video_ids,
           GROUP_CONCAT(content_type, ',') as content_types,
           GROUP_CONCAT(live_status, ',') as live_statuses,
           GROUP_CONCAT(CAST(is_premiere AS TEXT), ',') as premieres
    FROM videos
    WHERE source = 'youtube'
    GROUP BY title, channel_name
    HAVING cnt > 1
    ORDER BY cnt DESC
    LIMIT ?
    ''', (limit,))

    duplicate_groups = [dict(row) for row in cursor.fetchall()]

    print("=== YouTubeã®é‡è¤‡ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å‹•ç”» ===\n")

    if not duplicate_groups:
        print("é‡è¤‡ç™»éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ âœ…\n")
    else:
        for row in duplicate_groups:
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {row['title'][:60]}")
            print(f"  ç™»éŒ²æ•°: {row['cnt']}")
            print(f"  video_ids: {row['video_ids']}")
            print(f"  content_types: {row['content_types']}")
            print(f"  live_statuses: {row['live_statuses']}")
            print(f"  premieres: {row['premieres']}")
            print()

    # ============================================
    # 2. åŒã˜video_idã§è¤‡æ•°ã®live_statusã‚’æŒã¤ã‚±ãƒ¼ã‚¹
    # ============================================
    cursor.execute('''
    SELECT video_id, title, COUNT(*) as cnt,
           GROUP_CONCAT(DISTINCT live_status) as live_statuses,
           GROUP_CONCAT(DISTINCT content_type) as content_types
    FROM videos
    WHERE source = 'youtube'
    GROUP BY video_id
    HAVING cnt > 1
    ORDER BY cnt DESC
    LIMIT ?
    ''', (limit,))

    same_video_id_duplicates = [dict(row) for row in cursor.fetchall()]

    print("=== åŒã˜video_idã§è¤‡æ•°ã®live_statusã‚’æŒã¤ã‚±ãƒ¼ã‚¹ ===\n")

    if not same_video_id_duplicates:
        print("åŒä¸€video_idã®é‡è¤‡ã¯ã‚ã‚Šã¾ã›ã‚“ âœ…\n")
    else:
        for row in same_video_id_duplicates:
            print(f"Video ID: {row['video_id']}")
            print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {row['title'][:60]}")
            print(f"  ç™»éŒ²æ•°: {row['cnt']}")
            print(f"  live_statuses: {row['live_statuses']}")
            print(f"  content_types: {row['content_types']}")
            print()

    conn.close()

    return {
        'duplicate_groups': duplicate_groups,
        'same_video_id_duplicates': same_video_id_duplicates
    }


def cleanup_youtube_duplicates_with_priority(db_path=None):
    """
    YouTubeé‡è¤‡ç™»éŒ²å‹•ç”»ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå„ªå…ˆåº¦ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨ï¼‰

    åŒã˜ã‚¿ã‚¤ãƒˆãƒ«+ãƒãƒ£ãƒ³ãƒãƒ«åã®å‹•ç”»ãŒè¤‡æ•°ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã€
    å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦æœ€å„ªå…ˆã®å‹•ç”»ã‚’ä¿æŒã—ã€ãã‚Œä»¥å¤–ã‚’å‰Šé™¤

    Args:
        db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•è§£æ±ºï¼‰

    Returns:
        dict: {
            'total_deleted': å‰Šé™¤ã—ãŸä»¶æ•°,
            'registered_to_cache': deleted_videos.json ã«ç™»éŒ²ã—ãŸä»¶æ•°,
            'duplicate_groups': å‡¦ç†ã—ãŸã‚°ãƒ«ãƒ¼ãƒ—æ•°
        }
    """
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    from youtube_core.youtube_dedup_priority import get_video_priority, select_best_video

    if db_path is None:
        db_path = _get_db_path()

    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    # deleted_video_cache ã‚’åˆæœŸåŒ–
    try:
        from deleted_video_cache import get_deleted_video_cache
        deleted_cache = get_deleted_video_cache()
    except ImportError:
        logger.warning("âš ï¸ deleted_video_cache ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        deleted_cache = None

    # åŒã˜ã‚¿ã‚¤ãƒˆãƒ«+ãƒãƒ£ãƒ³ãƒãƒ«åã§è¤‡æ•°ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å‹•ç”»ã‚’æ¤œå‡º
    cursor.execute('''
    SELECT title, channel_name, COUNT(*) as cnt,
           GROUP_CONCAT(id) as ids
    FROM videos
    WHERE source = 'youtube'
    GROUP BY title, channel_name
    HAVING cnt > 1
    ORDER BY cnt DESC
    ''')

    duplicate_groups = cursor.fetchall()
    print(f"=== YouTubeé‡è¤‡å‹•ç”»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå„ªå…ˆåº¦ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨ï¼‰===\n")
    print(f"é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(duplicate_groups)}\n")

    total_deleted = 0
    registered_to_cache = 0

    for group in duplicate_groups:
        title = group['title']
        channel_name = group['channel_name']
        cnt = group['cnt']
        ids = list(map(int, group['ids'].split(',')))

        print(f"ã€é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã€‘")
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {title[:60]}")
        print(f"  ãƒãƒ£ãƒ³ãƒãƒ«: {channel_name}")
        print(f"  ç™»éŒ²æ•°: {cnt}")

        # å„IDã®å‹•ç”»æƒ…å ±ã‚’å–å¾—
        videos = []
        for vid_id in ids:
            cursor.execute("""
                SELECT id, video_id, content_type, live_status, is_premiere, published_at
                FROM videos
                WHERE id=?
            """, (vid_id,))
            row = cursor.fetchone()
            if row:
                videos.append({
                    'id': row['id'],
                    'video_id': row['video_id'],
                    'content_type': row['content_type'],
                    'live_status': row['live_status'],
                    'is_premiere': row['is_premiere'],
                    'published_at': row['published_at']
                })

        # å„å‹•ç”»ã®å„ªå…ˆåº¦ã‚’è¡¨ç¤º
        print("  å‹•ç”»ã®å„ªå…ˆåº¦:")
        for v in videos:
            priority = get_video_priority(v)
            print(f"    ID={v['id']:3d}, video_id={v['video_id']}, type={v['content_type']:10s}, " +
                  f"live_status={str(v['live_status']):10s}, premiere={v['is_premiere']}, priority={priority[0]}")

        # æœ€å„ªå…ˆã®å‹•ç”»ã‚’é¸æŠ
        best_video = select_best_video(videos)
        best_priority = get_video_priority(best_video)

        print(f"  âœ… ä¿æŒ: ID={best_video['id']:3d}, video_id={best_video['video_id']} (priority={best_priority[0]})")

        # ãã‚Œä»¥å¤–ã‚’å‰Šé™¤
        deleted_count = 0
        for v in videos:
            if v['id'] != best_video['id']:
                cursor.execute("DELETE FROM videos WHERE id = ?", (v['id'],))
                priority = get_video_priority(v)
                print(f"  âŒ å‰Šé™¤: ID={v['id']:3d}, video_id={v['video_id']} (priority={priority[0]})")

                # deleted_videos.json ã«ç™»éŒ²
                if deleted_cache:
                    try:
                        deleted_cache.add_deleted_video(v['video_id'], source='youtube')
                        print(f"     ğŸ“Œ deleted_videos.json ã«ç™»éŒ²")
                        registered_to_cache += 1
                    except Exception as e:
                        print(f"     âš ï¸ ç™»éŒ²å¤±æ•—: {e}")

                deleted_count += 1
                total_deleted += 1

        print()

    conn.commit()
    conn.close()

    print(f"\n=== çµæœ ===")
    print(f"å‰Šé™¤ã—ãŸå‹•ç”»: {total_deleted}ä»¶")
    print(f"deleted_videos.json ã«ç™»éŒ²: {registered_to_cache}ä»¶")
    print(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—: {len(duplicate_groups)}ã‚°ãƒ«ãƒ¼ãƒ—")

    return {
        'total_deleted': total_deleted,
        'registered_to_cache': registered_to_cache,
        'duplicate_groups': len(duplicate_groups)
    }


if __name__ == "__main__":
    import sys

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆ
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "cleanup":
            cleanup_youtube_duplicates_with_priority()
        elif command == "check":
            check_duplicate_videos()
        else:
            print(f"ä½¿ç”¨æ–¹æ³•: python {sys.argv[0]} [check|cleanup]")
            sys.exit(1)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒã‚§ãƒƒã‚¯ + ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        check_duplicate_videos()
        print("\n" + "=" * 50 + "\n")
        cleanup_youtube_duplicates_with_priority()
